{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Week 7 - miao.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabioRovai/Personalisation-and-Machine-Learning/blob/main/assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqNzDzXfVn0z"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0VlgRzmKx8S",
        "outputId": "25ab9d6c-0d16-4a13-dc22-532d4952abbc"
      },
      "source": [
        "\n",
        "#I implemented an RNN trained on Movies classification tasks to evaluate if these reviews where good or not.\n",
        "#I stored it in a .py file, keeping this notebook as clean as possible.\n",
        "!wget 'https://victorzhou.com/movie-reviews-dataset.zip' -qq\n",
        "!unzip -qq '/content/movie-reviews-dataset.zip' \n",
        "!pip install deepctr -qq\n",
        "#move to main colab directory\n",
        "%cd '/content/'\n",
        "#I did a module conversion of this tutorial https://victorzhou.com/blog/keras-rnn-tutorial/ and stored it into a git.\n",
        "!wget 'https://raw.githubusercontent.com/FabioRovai/test7/main/untitled49.py'\n",
        "#change into correct directory, otherwse manually change train and test\n",
        "%cd '/content/movie-reviews-dataset'\n",
        "#run the module\n",
        "%run '/content/untitled49.py'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 122kB 5.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 8.6MB/s \n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h/content\n",
            "--2021-06-07 14:43:04--  https://raw.githubusercontent.com/FabioRovai/test7/main/untitled49.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2710 (2.6K) [text/plain]\n",
            "Saving to: ‘untitled49.py’\n",
            "\n",
            "untitled49.py       100%[===================>]   2.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-07 14:43:04 (58.0 MB/s) - ‘untitled49.py’ saved [2710/2710]\n",
            "\n",
            "/content/movie-reviews-dataset\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "b'The eight Jean Rollin film I have watched is also possibly the weirdest; the intriguing plot (such as it is) seems initially to be too flimsy to sustain even its trim 84 minutes but it somehow contrives to get inordinately muddled as it goes along! A would-be female vampire (scantily-clad, as promised by the title) is held in captivity inside a remote ch\\xc3\\xa2teau and emerges only to \\'feast\\' on the blood of willing victims (who are apparently members of a suicide club) As if unsure where all of this would lead him, the writer-director ultimately has the human villain \\xc2\\x96 actually the blank-faced hero\\'s kinky father \\xc2\\x96 ludicrously revealed as a mutant(?!) from the future! The languorous pace and dream-like atmosphere (the cultists wear hoods and animal masks to hide their features from the sheltered girl) are, of course, typical of both the film-maker (ditto the seashore setting at the {anti}climax) and the \"Euro-Cult\" style, as are the bevy of nubile beauties on display. Personally, the most enjoyable thing about the whole visually attractive but intellectually vacuous affair was watching familiar character actor Bernard Musson (who appeared in six latter-day Luis Bunuel films) crop up bemusedly through it from time to time!'\n",
            "0\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 19s 16ms/step - loss: 0.5549 - accuracy: 0.7118\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4415 - accuracy: 0.7977\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.4072 - accuracy: 0.8157\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3803 - accuracy: 0.8334\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3612 - accuracy: 0.8433\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3427 - accuracy: 0.8523\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3255 - accuracy: 0.8627\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3145 - accuracy: 0.8677\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.3021 - accuracy: 0.8725\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.2935 - accuracy: 0.8778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNFgLUnGK7Ae",
        "outputId": "b6e01a43-b6e7-4f85-c6b9-255ef156b36e"
      },
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 8s 10ms/step - loss: 0.5781 - accuracy: 0.7764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5780869722366333, 0.7763599753379822]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTzjpDFjLBJm",
        "outputId": "dda4f124-e049-475d-bef0-474f989ec7fe"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "##Load the data \n",
        "#If raw token expires upload manually upload the dataset, is a bug or a security protocol from git.arts\n",
        "df = pd.read_json(\"https://git.arts.ac.uk/raw/lmccallum/Personalisation/master/data/renttherunway_cleaned.json?token=AAAAAH7XGLI4G4S6DYUDL5DAYPTSO\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "#df[['review_text']]\n",
        "#Select random result \n",
        "foo=df[['review_text']].sample()\n",
        "print (foo)\n",
        "print(model.predict([foo]))\n",
        "foo=df[['review_text']]\n",
        "#creating a new section for the review scores\n",
        "df[\"review_score\"] = model.predict([foo]) \n",
        "#print(df[\"review_score\"].describe())\n",
        "#df[\"rating\"].describe()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                                  review_text\n",
            "98099  Very flattering fit. A tad uncomfortable but totally worth it :)  Felt like my wedding day all over again! Wore 3 inch heels in the size 4, long length (I'm 5'6) and it was perfect. \n",
            "[[0.8157796]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "DGWUqGZNVn0z",
        "outputId": "52afc5c9-6fd1-4e1b-d1dc-ba1df00a3fc5"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fit</th>\n",
              "      <th>user_id</th>\n",
              "      <th>bust size</th>\n",
              "      <th>item_id</th>\n",
              "      <th>weight</th>\n",
              "      <th>rating</th>\n",
              "      <th>rented for</th>\n",
              "      <th>review_text</th>\n",
              "      <th>body type</th>\n",
              "      <th>review_summary</th>\n",
              "      <th>category</th>\n",
              "      <th>height</th>\n",
              "      <th>size</th>\n",
              "      <th>age</th>\n",
              "      <th>review_date</th>\n",
              "      <th>review_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fit</td>\n",
              "      <td>420272</td>\n",
              "      <td>34d</td>\n",
              "      <td>2260466</td>\n",
              "      <td>137lbs</td>\n",
              "      <td>10.0</td>\n",
              "      <td>vacation</td>\n",
              "      <td>An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.</td>\n",
              "      <td>hourglass</td>\n",
              "      <td>So many compliments!</td>\n",
              "      <td>romper</td>\n",
              "      <td>5' 8\"</td>\n",
              "      <td>14</td>\n",
              "      <td>28.0</td>\n",
              "      <td>April 20, 2016</td>\n",
              "      <td>0.813959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fit</td>\n",
              "      <td>273551</td>\n",
              "      <td>34b</td>\n",
              "      <td>153475</td>\n",
              "      <td>132lbs</td>\n",
              "      <td>10.0</td>\n",
              "      <td>other</td>\n",
              "      <td>I rented this dress for a photo shoot. The theme was \"Hollywood Glam and Big Beautiful Hats\". The dress was very comfortable and easy to move around in. It is definitely on my list to rent again for another formal event.</td>\n",
              "      <td>straight &amp; narrow</td>\n",
              "      <td>I felt so glamourous!!!</td>\n",
              "      <td>gown</td>\n",
              "      <td>5' 6\"</td>\n",
              "      <td>12</td>\n",
              "      <td>36.0</td>\n",
              "      <td>June 18, 2013</td>\n",
              "      <td>0.815930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fit  user_id bust size  item_id  ... size   age     review_date review_score\n",
              "0  fit   420272       34d  2260466  ...   14  28.0  April 20, 2016     0.813959\n",
              "1  fit   273551       34b   153475  ...   12  36.0   June 18, 2013     0.815930\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ZrCy3XVn00",
        "outputId": "82c52711-f6be-40f4-c61b-745c55ca5988"
      },
      "source": [
        "#How many interactions in total, how many unique users, how many unique items \n",
        "len(df), len(df[\"user_id\"].unique()), len(df[\"item_id\"].unique())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192544, 105571, 5850)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-WIm_OjVn01"
      },
      "source": [
        "### Dates \n",
        "\n",
        "Since sequence is important to us, we're going to need to sort the data by review date. Currently, ``Pandas`` sees this column as an ``object``, so we'll use ``pd.to_datetime()`` to convert the string to a date. We can then sort it, and do maths with it! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Kz4TpTVn01"
      },
      "source": [
        "#Parse the dates\n",
        "df[\"review_date\"] = pd.to_datetime(df[\"review_date\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9o07scVn01",
        "outputId": "05992f23-75b6-440f-ad4d-f1fd20a10870"
      },
      "source": [
        "#Confirm the data types are correct\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fit                       object\n",
              "user_id                    int64\n",
              "bust size                 object\n",
              "item_id                    int64\n",
              "weight                    object\n",
              "rating                   float64\n",
              "rented for                object\n",
              "review_text               object\n",
              "body type                 object\n",
              "review_summary            object\n",
              "category                  object\n",
              "height                    object\n",
              "size                       int64\n",
              "age                      float64\n",
              "review_date       datetime64[ns]\n",
              "review_score             float32\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3iqigAVn02"
      },
      "source": [
        "#Sort\n",
        "df = df.sort_values(by=\"review_date\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt1YPTcCRVYh"
      },
      "source": [
        "df_copy=df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYz3EXXiVn02"
      },
      "source": [
        "### Getting the User Sequence \n",
        "\n",
        "Currently, our dataset has each item on a separate row. In order to get the sequence of purchase for each user, we need to format the data. \n",
        "\n",
        "We also take the date in its absolute form and change it to ``days since end of dataset`` (the most recent rental being ``0 days``)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uANZkinzVn03"
      },
      "source": [
        "users = df[\"user_id\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k78eEtCVn03"
      },
      "source": [
        "data = []\n",
        "last_date = df[\"review_date\"].max()\n",
        "\n",
        "for user in users:\n",
        "    ##Get all the items for that user\n",
        "    rows = df[df[\"user_id\"]==user]\n",
        "    \n",
        "    #Get all the item_ids\n",
        "    items = rows[\"item_id\"]\n",
        "    #Added 2 new features\n",
        "    #get all the review score\n",
        "    review_score = rows[\"review_score\"]\n",
        "    #get all the rating\n",
        "    rating   = rows[\"rating\"]\n",
        " \n",
        "    \n",
        "    #Get all the dates\n",
        "    days_since_now = (last_date - rows[\"review_date\"])\n",
        "    days_since_now = np.array([i.days for i in days_since_now])\n",
        "    \n",
        "    #Collect into a dictionary for each user\n",
        "    data.append({\"item_id\":items.values,\"nb_days\":days_since_now, \"rating\":rating.values, \"reviews\":review_score.values })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSWTK-4FVn03"
      },
      "source": [
        "#Convert to a dataframe and save\n",
        "data = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-L8cZBTVMSb"
      },
      "source": [
        "#Made a copy\n",
        "magic=data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRvy-6pVAa5"
      },
      "source": [
        "#adjusting notation\n",
        "magic[\"reviews\"] = 10 * magic[\"reviews\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxO4Fkg5Vn04"
      },
      "source": [
        "#data\n",
        "#magic\n",
        "#Changed name\n",
        "data=magic.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "59spVTvrXrp7",
        "outputId": "c85637f7-fe34-4e64-e97a-8a8bf53a73b4"
      },
      "source": [
        "magic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>nb_days</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[125564, 183200]</td>\n",
              "      <td>[2623, 1383]</td>\n",
              "      <td>[10.0, 10.0]</td>\n",
              "      <td>[8.157551, 0.37812352]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[126335, 132738, 130259]</td>\n",
              "      <td>[2520, 2096, 1745]</td>\n",
              "      <td>[8.0, 10.0, 10.0]</td>\n",
              "      <td>[8.97586, 8.940669, 7.3773484]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[126335]</td>\n",
              "      <td>[2511]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[7.668132]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[125564, 240137, 468020]</td>\n",
              "      <td>[2510, 848, 848]</td>\n",
              "      <td>[10.0, 10.0, 10.0]</td>\n",
              "      <td>[8.123618, 8.160177, 8.107747]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[190529]</td>\n",
              "      <td>[2500]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[8.008156]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105566</th>\n",
              "      <td>[454564]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[8.0]</td>\n",
              "      <td>[8.168954]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105567</th>\n",
              "      <td>[1498329]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[8.150664]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105568</th>\n",
              "      <td>[2835159]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[1.2147732]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105569</th>\n",
              "      <td>[1969604]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[6.951802]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105570</th>\n",
              "      <td>[2726766]</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[8.189022]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>105571 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         item_id  ...                         reviews\n",
              "0               [125564, 183200]  ...          [8.157551, 0.37812352]\n",
              "1       [126335, 132738, 130259]  ...  [8.97586, 8.940669, 7.3773484]\n",
              "2                       [126335]  ...                      [7.668132]\n",
              "3       [125564, 240137, 468020]  ...  [8.123618, 8.160177, 8.107747]\n",
              "4                       [190529]  ...                      [8.008156]\n",
              "...                          ...  ...                             ...\n",
              "105566                  [454564]  ...                      [8.168954]\n",
              "105567                 [1498329]  ...                      [8.150664]\n",
              "105568                 [2835159]  ...                     [1.2147732]\n",
              "105569                 [1969604]  ...                      [6.951802]\n",
              "105570                 [2726766]  ...                      [8.189022]\n",
              "\n",
              "[105571 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFn93EVHVn04"
      },
      "source": [
        "### Make the Dataset \n",
        "\n",
        "Now we need to make the actual **training set** that we will use for our model. \n",
        "\n",
        "Remember, the purpose of this model is to learn to predict the **next item rented in the sequence**. So for our input we will have \n",
        "\n",
        " * [item1, item2, item3, item4, ..., itemN]\n",
        " \n",
        " * [days1, days2, days3, days4, ..., daysN]\n",
        " \n",
        " \n",
        "And for our output we will have **the next item in the sequence** \n",
        "  \n",
        " \n",
        " * [itemN+1]\n",
        " \n",
        " \n",
        "We also need our sequences to all be **of equal length**, and because this dataset doesn't have loads of really long sequences, we don't want to throw away stuff below a threshold! So, instead we **zero padd** the end of each sequence if its not as long as the maximum length we have picked (in this case ``5``)\n",
        "\n",
        "* [0, 0, item1, item2, ..., itemN]\n",
        " \n",
        "* [0, 0, days1, days2, ..., daysN]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK2CLdiHVn05"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZs-gkMLVn05"
      },
      "source": [
        "#split into examples of 5 last things + 1, if less than 5, zero pad\n",
        "training_set = []\n",
        "max_len = 5\n",
        "for _, user in data.iterrows():\n",
        "    i = 0\n",
        "    #Get all items for each user \n",
        "    items = user[\"item_id\"]\n",
        "    num_items = len(items)\n",
        "    #If only one item purchased, there is no sequence! We need at least 2\n",
        "    #I've found better accuracy limiting to min 3\n",
        "    if num_items > 2:\n",
        "        nb_days = np.array(user[\"nb_days\"])\n",
        "        #Added rating and reviews\n",
        "        rating = np.array(user[\"rating\"])\n",
        "        review_score = np.array(user[\"reviews\"])\n",
        "        end = False\n",
        "        #Cycle over items, taking windows of 5 and moving forwards by 1 each time\n",
        "        while not end:\n",
        "            \n",
        "            #If we're off the end of the list, break out of the loop\n",
        "            target = i + max_len\n",
        "            if target >= num_items - 1:\n",
        "                end = True\n",
        "                target = num_items - 1\n",
        "            \n",
        "            #Get the input item and day features, zero padding\n",
        "            input_items = pad_sequences([items[i:target]], max_len)[0]\n",
        "            days = pad_sequences([nb_days[i:target]], max_len)[0]\n",
        "            input_rating = pad_sequences([rating[i:target]], max_len)[0]\n",
        "            input_rewiews = pad_sequences([review_score[i:target]], max_len)[0]\n",
        "            \n",
        "            #Get the adjusted seqeunce (shifted by one) for the target\n",
        "            target_items = np.concatenate((input_items[1:],[items[target]]))\n",
        "            #target_items = [items[target]]\n",
        "            \n",
        "            #Add to the dataset\n",
        "            row = {\n",
        "                \"item_id\":input_items,\n",
        "                \"nb_days\":days,\n",
        "                \"target\":target_items,\n",
        "                #Added rating and reviews\n",
        "                \"rating\":input_rating,\n",
        "                \"reviews\":input_rewiews,\n",
        "            }\n",
        "            training_set.append(row)\n",
        "            \n",
        "            #Increment pointer\n",
        "            i = i + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yffqJ6T7Vn07"
      },
      "source": [
        "training_set = pd.DataFrame(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv7UH7IQVn07",
        "outputId": "080b3770-441d-47ca-e643-7a67460f3f64"
      },
      "source": [
        "len(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "Fz2cSMLnVn08",
        "outputId": "4bbbd5f3-6885-4256-b6c9-d3b8e7c2e6a0"
      },
      "source": [
        "training_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>nb_days</th>\n",
              "      <th>target</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 0, 0, 126335, 132738]</td>\n",
              "      <td>[0, 0, 0, 2520, 2096]</td>\n",
              "      <td>[0, 0, 126335, 132738, 130259]</td>\n",
              "      <td>[0, 0, 0, 8, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 0, 0, 125564, 240137]</td>\n",
              "      <td>[0, 0, 0, 2510, 848]</td>\n",
              "      <td>[0, 0, 125564, 240137, 468020]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 0, 0, 126335, 531077]</td>\n",
              "      <td>[0, 0, 0, 2478, 992]</td>\n",
              "      <td>[0, 0, 126335, 531077, 253667]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 0, 126335, 1338469, 2261828]</td>\n",
              "      <td>[0, 0, 2474, 1105, 1105]</td>\n",
              "      <td>[0, 126335, 1338469, 2261828, 1846462]</td>\n",
              "      <td>[0, 0, 4, 8, 10]</td>\n",
              "      <td>[0, 0, 8, 8, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 126335, 180014, 833666]</td>\n",
              "      <td>[0, 0, 2458, 1119, 735]</td>\n",
              "      <td>[0, 126335, 180014, 833666, 640839]</td>\n",
              "      <td>[0, 0, 10, 10, 10]</td>\n",
              "      <td>[0, 0, 7, 8, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34583</th>\n",
              "      <td>[0, 0, 0, 715164, 2720289]</td>\n",
              "      <td>[0, 0, 0, 4, 4]</td>\n",
              "      <td>[0, 0, 715164, 2720289, 2665815]</td>\n",
              "      <td>[0, 0, 0, 10, 8]</td>\n",
              "      <td>[0, 0, 0, 8, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34584</th>\n",
              "      <td>[0, 0, 0, 872442, 322704]</td>\n",
              "      <td>[0, 0, 0, 4, 4]</td>\n",
              "      <td>[0, 0, 872442, 322704, 844580]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34585</th>\n",
              "      <td>[0, 0, 2893615, 2072280, 2675545]</td>\n",
              "      <td>[0, 0, 4, 4, 4]</td>\n",
              "      <td>[0, 2893615, 2072280, 2675545, 1434889]</td>\n",
              "      <td>[0, 0, 8, 8, 10]</td>\n",
              "      <td>[0, 0, 1, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34586</th>\n",
              "      <td>[0, 0, 0, 1252971, 2945301]</td>\n",
              "      <td>[0, 0, 0, 4, 4]</td>\n",
              "      <td>[0, 0, 1252971, 2945301, 1793377]</td>\n",
              "      <td>[0, 0, 0, 10, 8]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34587</th>\n",
              "      <td>[0, 0, 0, 2237227, 2125981]</td>\n",
              "      <td>[0, 0, 0, 3, 3]</td>\n",
              "      <td>[0, 0, 2237227, 2125981, 2451150]</td>\n",
              "      <td>[0, 0, 0, 8, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34588 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 item_id  ...          reviews\n",
              "0              [0, 0, 0, 126335, 132738]  ...  [0, 0, 0, 8, 8]\n",
              "1              [0, 0, 0, 125564, 240137]  ...  [0, 0, 0, 8, 8]\n",
              "2              [0, 0, 0, 126335, 531077]  ...  [0, 0, 0, 8, 8]\n",
              "3       [0, 0, 126335, 1338469, 2261828]  ...  [0, 0, 8, 8, 3]\n",
              "4         [0, 0, 126335, 180014, 833666]  ...  [0, 0, 7, 8, 7]\n",
              "...                                  ...  ...              ...\n",
              "34583         [0, 0, 0, 715164, 2720289]  ...  [0, 0, 0, 8, 1]\n",
              "34584          [0, 0, 0, 872442, 322704]  ...  [0, 0, 0, 8, 8]\n",
              "34585  [0, 0, 2893615, 2072280, 2675545]  ...  [0, 0, 1, 8, 8]\n",
              "34586        [0, 0, 0, 1252971, 2945301]  ...  [0, 0, 0, 8, 8]\n",
              "34587        [0, 0, 0, 2237227, 2125981]  ...  [0, 0, 0, 8, 8]\n",
              "\n",
              "[34588 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD363dqaVn08"
      },
      "source": [
        "## Feature Engineering \n",
        "\n",
        "### Item Ids\n",
        "\n",
        "Currently the ``item_ids`` are arbitrary and relate to the **Rent the Runway** catalogue. We are going to encode the in an **embedding** so need to make them indexes from ``0 -> num_items``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZzvB5eoVn09"
      },
      "source": [
        "#Get all the unique items from the data set\n",
        "all_input_items = np.array([i for i in training_set[\"item_id\"].values]).flatten()\n",
        "all_target_items = np.array([i for i in training_set[\"target\"].values]).flatten()\n",
        "all_items = np.concatenate((all_input_items, all_target_items))\n",
        "unique_items = np.unique(all_items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snn-Hv7LVn09"
      },
      "source": [
        "#Make a look up dictionary from item_id to index\n",
        "item_to_index = {item_id:i for i, item_id in enumerate(unique_items)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oJAPUNAVn09"
      },
      "source": [
        "#Swap out the new ids and convert to 2d arrays for use in training \n",
        "item_indexes = np.array([np.array([item_to_index[i] for i in ids], dtype=int) for ids in training_set[\"item_id\"]], dtype=int)\n",
        "target_indexes = np.array([np.array([item_to_index[i] for i in ids], dtype=int) for ids in training_set[\"target\"]], dtype=int)\n",
        "days = np.array([np.array([j for j in i],dtype=int) for i in training_set[\"nb_days\"]],dtype=int)\n",
        "#Added rating and reviews and converted in int.\n",
        "rating = np.array([np.array([k for k in i],dtype=int) for i in training_set[\"rating\"]],dtype=int)\n",
        "reviews = np.array([np.array([l for l in i],dtype=int) for i in training_set[\"reviews\"]],dtype=int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vhfFsz9Vn0-"
      },
      "source": [
        "### Bucketting Dates\n",
        "\n",
        "One interesting approach they have taken at Decathalon is to **bucket** days and then **learn an embedding**, almost treating it as a **categorical variable**. \n",
        "\n",
        "To do this, we use ``tf.keras.layers.experimental.preprocessing.Discretization()`` to separate the days into **100 equally spaced bins**, with one more for the **zero padding** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viEEgasSVn0-"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_PIPStPVn0-",
        "scrolled": true,
        "outputId": "72ed790d-42b7-4710-d9a1-eb221fb12183"
      },
      "source": [
        "#Get min and max\n",
        "days_min = days.min()\n",
        "days_max = days.max()\n",
        "\n",
        "#Generate 100 equally spaced boundaries \n",
        "boundaries = list(np.linspace(days_min, days_max,100,dtype=int))\n",
        "boundaries.insert(0,0)\n",
        "boundaries[1] = 1\n",
        "\n",
        "#Bucket the day features\n",
        "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "    bins=boundaries)\n",
        "bucket_days = discretize_layer(days).numpy() #- 1\n",
        "bucket_days = bucket_days\n",
        "#Bucket the rating features\n",
        "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "    bins=boundaries)\n",
        "bucket_rat = discretize_layer(rating).numpy() #- 1\n",
        "bucket_rat = bucket_rat\n",
        "\n",
        "#Bucket the reviews features\n",
        "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "    bins=boundaries)\n",
        "bucket_rev = discretize_layer(reviews).numpy() #- 1\n",
        "bucket_rev = bucket_rev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSqA57zMVn0_",
        "outputId": "fca9fac6-733f-409b-9c88-78a9a9dd1599"
      },
      "source": [
        "days"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, 2520, 2096],\n",
              "       [   0,    0,    0, 2510,  848],\n",
              "       [   0,    0,    0, 2478,  992],\n",
              "       ...,\n",
              "       [   0,    0,    4,    4,    4],\n",
              "       [   0,    0,    0,    4,    4],\n",
              "       [   0,    0,    0,    3,    3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "898dyXv2Vn0_",
        "outputId": "cefbbfa6-49fd-49a0-a3f3-5062bd8dbeba"
      },
      "source": [
        "bucket_days"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, 100,  84],\n",
              "       [  0,   0,   0, 100,  35],\n",
              "       [  0,   0,   0,  99,  40],\n",
              "       ...,\n",
              "       [  0,   0,   2,   2,   2],\n",
              "       [  0,   0,   0,   2,   2],\n",
              "       [  0,   0,   0,   2,   2]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSMFQ185Vn1A"
      },
      "source": [
        "## Training and Validation Sets \n",
        "\n",
        "Next we split the data into training and validation sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9TtuAOhVn1A"
      },
      "source": [
        "#Generate and shuffle the indexes\n",
        "total = len(training_set)\n",
        "all_indexes = np.arange(total)\n",
        "np.random.shuffle(all_indexes)\n",
        "\n",
        "#Split the indexes\n",
        "split = 0.9\n",
        "train_indexes = all_indexes[:int(total*split)]\n",
        "test_indexes = all_indexes[int(total*split):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC15KNcbVn1B"
      },
      "source": [
        "#Make a dictionary for the inputs \n",
        "train_x = {'item_id': item_indexes[train_indexes],\n",
        "          'nb_days': bucket_days[train_indexes],\n",
        "           #Added rating and reviews\n",
        "           'rating': bucket_rat[train_indexes],\n",
        "           'reviews':bucket_rev[train_indexes]}\n",
        "\n",
        "test_x = {'item_id': item_indexes[test_indexes],\n",
        "          'nb_days': bucket_days[test_indexes],\n",
        "          #Added rating and reviews\n",
        "          'rating': bucket_rat[test_indexes],\n",
        "          'reviews':bucket_rev[test_indexes]}\n",
        "\n",
        "#Make an array for the outputs \n",
        "train_y = target_indexes[train_indexes]\n",
        "test_y = target_indexes[test_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-xAsUPwcWe0",
        "outputId": "3d3a6c4f-2b90-42df-c8a0-372ea4e23281"
      },
      "source": [
        "test_y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0, 3489, 3609, 5365, 5490],\n",
              "       [3148,    2, 2248, 4289, 5023],\n",
              "       [5597, 4628, 5295, 4091, 4613],\n",
              "       ...,\n",
              "       [ 656, 1160, 1634, 3240, 1907],\n",
              "       [3465, 2339, 1331,  659, 2861],\n",
              "       [1016, 2968, 1516, 2234,  956]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrcsLGBUVn1B",
        "outputId": "ab6bc79c-97af-4d7f-f827-e00c5651577c"
      },
      "source": [
        "test_x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'item_id': array([[   0,    0, 3489, 3609, 5365],\n",
              "        [   0, 3148,    2, 2248, 4289],\n",
              "        [3476, 5597, 4628, 5295, 4091],\n",
              "        ...,\n",
              "        [1921,  656, 1160, 1634, 3240],\n",
              "        [2738, 3465, 2339, 1331,  659],\n",
              "        [1544, 1016, 2968, 1516, 2234]]),\n",
              " 'nb_days': array([[ 0,  0, 19, 19,  4],\n",
              "        [ 0, 36, 32, 30, 28],\n",
              "        [11, 11, 11,  6,  6],\n",
              "        ...,\n",
              "        [30, 30, 29, 22, 22],\n",
              "        [24, 24, 23, 16, 16],\n",
              "        [22, 20, 20, 17, 14]], dtype=int32),\n",
              " 'rating': array([[0, 0, 2, 2, 2],\n",
              "        [0, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        ...,\n",
              "        [2, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2]], dtype=int32),\n",
              " 'reviews': array([[0, 0, 0, 2, 2],\n",
              "        [0, 0, 2, 2, 2],\n",
              "        [0, 2, 2, 2, 2],\n",
              "        ...,\n",
              "        [0, 0, 2, 2, 2],\n",
              "        [2, 2, 2, 1, 2],\n",
              "        [2, 1, 2, 2, 2]], dtype=int32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXeOYNvVn1C"
      },
      "source": [
        "## Build and Train the Model \n",
        "\n",
        "Previously we have used ``Keras's Sequential API``, where we first make a model then **sequentially add layers to it** one by one. This works because each layer only has one input, and outputs directly into the next. \n",
        "\n",
        "However, whilst this is broadly true of our model, its not strictly true. Our **Embedding layers** both feed into the **Concatenate layer**. So instead, we will use the [Keras Functional API](https://keras.io/guides/functional_api/).\n",
        "\n",
        "As the documentation says \n",
        "\n",
        "```\n",
        "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
        "```\n",
        "\n",
        "Essentially, when we build up the ``network graph``, instead of **adding things to the model**, we **specify input layer** we want this new layer to have. \n",
        "\n",
        "Here, we make a layer (``x``), and then a layer(``output``), specifying that ``x`` is in the input for ``output``\n",
        "\n",
        "```\n",
        "x = layers.Dense(64, activation=\"relu\")\n",
        "outputs = layers.Dense(10)(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoIw1OBBVn1C"
      },
      "source": [
        "#Model hyper parameters \n",
        "item_vocab_size = len(unique_items)\n",
        "hp = {\n",
        "    \"embedding_item\":100,\n",
        "    \"embedding_nb_days\":20,\n",
        "    #Added rating and reviews\n",
        "    \"embedding_rating\":20,\n",
        "    \"embedding_reviews\":20,\n",
        "    \"rnn_units_cat\":[1024,512],\n",
        "    \"learning_rate\":0.01\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMNgE5CfVn1C"
      },
      "source": [
        "### ``tf.keras.Input()``\n",
        "\n",
        "The [Input Layer](https://keras.io/api/layers/core_layers/input/) sits at the base of our ``Model`` and takes does what it says on the tin - Takes the input. \n",
        "\n",
        "We specfiy a dictionary of inputs to match our dictionaries we made in the **training set**. This means we have **two separate inputs**, that feed into **two separate embedding layers**.\n",
        "\n",
        "We give them a ``batch_input_shape`` of ``[None, max_len]``, so that it know each batch will be a sequence of ``max_len`` (in our ase 5) items, but the batch_size itself isn't decided until we ``compile()`` the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Epv29opVn1D"
      },
      "source": [
        "inputs = {}\n",
        "inputs['item_id'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                   name='item_id', dtype=tf.int32)\n",
        "\n",
        "# nb_days bucketized\n",
        "inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                   name='nb_days', dtype=tf.int32)\n",
        "#rating\n",
        "inputs['rating'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                name='rating', dtype=tf.int32)\n",
        "#reviews   \n",
        "inputs['reviews'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                   name='reviews', dtype=tf.int32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL89VdErVn1D"
      },
      "source": [
        "### ``tf.keras.layers.Embedding()``\n",
        "\n",
        "Then we add the embedding layers, each time specifying which item from the **input dictionary** to take as input. \n",
        "\n",
        "The **item embedding** takes an input the size of the **number of unique items (vocab size)** and learns a mapping to a denser embedding of a given size. \n",
        "\n",
        "The **days embedding** takes an input the size of the **number of buckets + 1 (for zero padding)** and learns a mapping to a denser embedding of a given size. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOWrZiXwVn1E"
      },
      "source": [
        "embedding_item = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
        "                                           output_dim=hp.get('embedding_item'),\n",
        "                                           name='embedding_item'\n",
        "                                          )(inputs['item_id'])\n",
        "# nbins=100, +1 for zero padding\n",
        "embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\n",
        "                                              output_dim=hp.get('embedding_nb_days'),\n",
        "                                              name='embedding_nb_days'\n",
        "                                             )(inputs['nb_days'])\n",
        "\n",
        "#Added rating and reviews\n",
        "embedding_rating = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
        "                                           output_dim=hp.get('embedding_rating'),\n",
        "                                           name='embedding_rating'\n",
        "                                          )(inputs['rating'])\n",
        "\n",
        "\n",
        "embedding_reviews = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
        "                                           output_dim=hp.get('embedding_reviews'),\n",
        "                                           name='embedding_reviews'\n",
        "                                          )(inputs['reviews'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIwLV4SbsaSx",
        "outputId": "f6356d55-9f80-4357-9e24-74ba6fe29bf4"
      },
      "source": [
        "print(embedding_item,embedding_nb_days,embedding_rating,embedding_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 5, 100), dtype=tf.float32, name=None), name='embedding_item/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_item'\") KerasTensor(type_spec=TensorSpec(shape=(None, 5, 20), dtype=tf.float32, name=None), name='embedding_nb_days/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_nb_days'\") KerasTensor(type_spec=TensorSpec(shape=(None, 5, 20), dtype=tf.float32, name=None), name='embedding_rating/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_rating'\") KerasTensor(type_spec=TensorSpec(shape=(None, 5, 20), dtype=tf.float32, name=None), name='embedding_reviews/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_reviews'\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5h6cEYeVn1E"
      },
      "source": [
        "### `` tf.keras.layers.Concatenate()``\n",
        "\n",
        "We then concatentate embedding layers into one layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4RnK07sVn1F"
      },
      "source": [
        "# Concatenate embedding layers\n",
        "concat_embedding_input = tf.keras.layers.Concatenate(\n",
        " name='concat_embedding_input')([embedding_item, embedding_nb_days,embedding_rating, embedding_reviews  ]) \n",
        "\n",
        "concat_embedding_input = tf.keras.layers.BatchNormalization(\n",
        " name='batchnorm_inputs')(concat_embedding_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v65054oVn1F"
      },
      "source": [
        "### LSTM Layers \n",
        "\n",
        "When then put in the ``tf.keras.layers.LSTM()`` layer, with a ``tf.keras.layers.BatchNormalization()`` either side. \n",
        "\n",
        "More on that in the lecture!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFRLrZQ2Vn1F"
      },
      "source": [
        "input_layer = concat_embedding_input\n",
        "\n",
        "for i, num_units in enumerate(hp.get('rnn_units_cat')):\n",
        "    \n",
        "    # LSTM layer\n",
        "    rnn = tf.keras.layers.LSTM(units=num_units,\n",
        "                                   return_sequences=True,\n",
        "                                   recurrent_initializer='glorot_normal',\n",
        "                                   name='LSTM_cat' + str(i)\n",
        "                                   )(input_layer)\n",
        "\n",
        "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm' + str(i))(rnn)\n",
        "    \n",
        "    input_layer = rnn\n",
        "\n",
        "# create encoding padding mask\n",
        "encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['item_id'], 0))\n",
        "\n",
        "# Self attention so key=value in inputs\n",
        "att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
        "                                name='attention')(inputs=[rnn, rnn],\n",
        "                                                  mask=[encoding_padding_mask,\n",
        "                                                        encoding_padding_mask])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R0UKQi7Vn1G"
      },
      "source": [
        "### The Output\n",
        "\n",
        "Finally, we bring it all together in a ``tf.keras.layers.Dense()`` **softmax layer**. This means that the output of this layer will be \n",
        "\n",
        "``\n",
        "[batch_size x max_len x item_vocab_size]\n",
        "``\n",
        "\n",
        "Where each sequence in the batch is a ``[max_len x item_vocab_size]`` tensor telling us the probability of that item in the catalogue being next in the sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfBuqRjEVn1G"
      },
      "source": [
        "# Last layer is a fully connected one\n",
        "output = tf.keras.layers.Dense(item_vocab_size, activation = tf.nn.softmax, name='output')(att)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqvWaDxVVn1G"
      },
      "source": [
        "### Loss Function \n",
        "\n",
        "We write a custom loss function. This is necessary because we need to again mask out the 0s to stop the model optimising towards them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBGTQC9Vn1G"
      },
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "def loss_function(real, pred):\n",
        "    loss = SparseCategoricalCrossentropy()(real, pred)\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXUIR2_cVn1H"
      },
      "source": [
        "### ``tf.keras.Model()``\n",
        "\n",
        "Finally, we're ready to join the ``Inputs`` and the ``Outputs`` into a ``Model()`` object and ``compile()``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF98oCMBVn1H"
      },
      "source": [
        "model = tf.keras.Model(inputs, output)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "    loss=loss_function,\n",
        "    metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU-xvJrXVn1H",
        "outputId": "fffe0b70-2ffe-4a16-9513-85ca7d07f215"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "item_id (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "nb_days (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rating (InputLayer)             [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reviews (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_item (Embedding)      (None, 5, 100)       570100      item_id[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_nb_days (Embedding)   (None, 5, 20)        2020        nb_days[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_rating (Embedding)    (None, 5, 20)        114020      rating[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_reviews (Embedding)   (None, 5, 20)        114020      reviews[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concat_embedding_input (Concate (None, 5, 160)       0           embedding_item[0][0]             \n",
            "                                                                 embedding_nb_days[0][0]          \n",
            "                                                                 embedding_rating[0][0]           \n",
            "                                                                 embedding_reviews[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batchnorm_inputs (BatchNormaliz (None, 5, 160)       640         concat_embedding_input[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "LSTM_cat0 (LSTM)                (None, 5, 1024)      4853760     batchnorm_inputs[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batchnorm_lstm0 (BatchNormaliza (None, 5, 1024)      4096        LSTM_cat0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "LSTM_cat1 (LSTM)                (None, 5, 512)       3147776     batchnorm_lstm0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.equal (TFOpLambda)      (None, 5)            0           item_id[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batchnorm_lstm1 (BatchNormaliza (None, 5, 512)       2048        LSTM_cat1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.logical_not (TFOpLambda (None, 5)            0           tf.math.equal[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention (Attention)           (None, 5, 512)       0           batchnorm_lstm1[0][0]            \n",
            "                                                                 batchnorm_lstm1[0][0]            \n",
            "                                                                 tf.math.logical_not[0][0]        \n",
            "                                                                 tf.math.logical_not[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 5, 5701)      2924613     attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,733,093\n",
            "Trainable params: 11,729,701\n",
            "Non-trainable params: 3,392\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua5xxDiIVn1I",
        "scrolled": true,
        "outputId": "1c3bf8ac-762c-46d3-82df-df2a8bf736c6"
      },
      "source": [
        "history = model.fit(train_x,train_y,\n",
        "                    epochs=20, \n",
        "                    verbose=1,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(test_x, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "61/61 [==============================] - 10s 103ms/step - loss: 6.2282 - sparse_categorical_accuracy: 0.0025 - val_loss: 6.4345 - val_sparse_categorical_accuracy: 2.7962e-04\n",
            "Epoch 2/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 5.5446 - sparse_categorical_accuracy: 0.0134 - val_loss: 6.0327 - val_sparse_categorical_accuracy: 0.0015\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 4.7696 - sparse_categorical_accuracy: 0.0877 - val_loss: 5.3802 - val_sparse_categorical_accuracy: 0.0196\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 3.8874 - sparse_categorical_accuracy: 0.2497 - val_loss: 4.6571 - val_sparse_categorical_accuracy: 0.1034\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 3.1920 - sparse_categorical_accuracy: 0.3950 - val_loss: 3.8665 - val_sparse_categorical_accuracy: 0.2780\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 2.7042 - sparse_categorical_accuracy: 0.4978 - val_loss: 3.4787 - val_sparse_categorical_accuracy: 0.3736\n",
            "Epoch 7/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 2.3674 - sparse_categorical_accuracy: 0.5674 - val_loss: 3.2614 - val_sparse_categorical_accuracy: 0.4368\n",
            "Epoch 8/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 2.1210 - sparse_categorical_accuracy: 0.6163 - val_loss: 3.1847 - val_sparse_categorical_accuracy: 0.4700\n",
            "Epoch 9/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 1.9425 - sparse_categorical_accuracy: 0.6490 - val_loss: 3.1361 - val_sparse_categorical_accuracy: 0.4949\n",
            "Epoch 10/20\n",
            "61/61 [==============================] - 6s 90ms/step - loss: 1.7928 - sparse_categorical_accuracy: 0.6767 - val_loss: 3.1220 - val_sparse_categorical_accuracy: 0.5094\n",
            "Epoch 11/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 1.6825 - sparse_categorical_accuracy: 0.6947 - val_loss: 3.1415 - val_sparse_categorical_accuracy: 0.5116\n",
            "Epoch 12/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 1.5976 - sparse_categorical_accuracy: 0.7046 - val_loss: 3.1536 - val_sparse_categorical_accuracy: 0.5177\n",
            "Epoch 13/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 1.5216 - sparse_categorical_accuracy: 0.7164 - val_loss: 3.2444 - val_sparse_categorical_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 1.4571 - sparse_categorical_accuracy: 0.7246 - val_loss: 3.2105 - val_sparse_categorical_accuracy: 0.5214\n",
            "Epoch 15/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 1.3994 - sparse_categorical_accuracy: 0.7350 - val_loss: 3.2998 - val_sparse_categorical_accuracy: 0.5192\n",
            "Epoch 16/20\n",
            "61/61 [==============================] - 5s 90ms/step - loss: 1.3589 - sparse_categorical_accuracy: 0.7397 - val_loss: 3.4110 - val_sparse_categorical_accuracy: 0.5172\n",
            "Epoch 17/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 1.3222 - sparse_categorical_accuracy: 0.7431 - val_loss: 3.4743 - val_sparse_categorical_accuracy: 0.5204\n",
            "Epoch 18/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 1.2779 - sparse_categorical_accuracy: 0.7530 - val_loss: 3.4416 - val_sparse_categorical_accuracy: 0.5149\n",
            "Epoch 19/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 1.2297 - sparse_categorical_accuracy: 0.7652 - val_loss: 3.5370 - val_sparse_categorical_accuracy: 0.5229\n",
            "Epoch 20/20\n",
            "61/61 [==============================] - 6s 91ms/step - loss: 1.1814 - sparse_categorical_accuracy: 0.7770 - val_loss: 3.5768 - val_sparse_categorical_accuracy: 0.5269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUOKqKRyVn1I"
      },
      "source": [
        "## Predictions \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58HPB7bjVn1J"
      },
      "source": [
        "#Predictions for whole test set\n",
        "results = model.predict(test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxxYY1E6Vn1J",
        "outputId": "1eb6cfd6-b2de-4fe2-bc34-f3d21b8a4be7"
      },
      "source": [
        "#exmples x max_len x vocab size\n",
        "results.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3459, 5, 5701)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o348KLAVVn1J",
        "outputId": "6955538d-26e1-455d-b39b-e8539678b90b"
      },
      "source": [
        "#max_len x vocab size\n",
        "results[100].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5701)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM5h7viSVn1K",
        "outputId": "06acc9e5-cad9-4406-b0b7-d13e653a1f8b"
      },
      "source": [
        "#Get index of highest prob in item vocab\n",
        "results[100][-1].sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzf6Q-SfVn1L",
        "outputId": "dc7a0062-b54b-4d60-8b5a-59f2963fbf1e"
      },
      "source": [
        "results[-1].argmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDXrvl2IVn1L"
      },
      "source": [
        "index_to_item = {v:k for k,v in item_to_index.items()}    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBBClMgjVn1L",
        "outputId": "23606714-17cf-4e89-cb7a-35f6525e0631"
      },
      "source": [
        "index_to_item[70]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "155381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkwIf77bCaCA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "8dc373ac-c51c-4016-9f12-d2bc89efafd7"
      },
      "source": [
        "training_set.head(72)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>nb_days</th>\n",
              "      <th>target</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 0, 0, 126335, 132738]</td>\n",
              "      <td>[0, 0, 0, 2520, 2096]</td>\n",
              "      <td>[0, 0, 126335, 132738, 130259]</td>\n",
              "      <td>[0, 0, 0, 8, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 0, 0, 125564, 240137]</td>\n",
              "      <td>[0, 0, 0, 2510, 848]</td>\n",
              "      <td>[0, 0, 125564, 240137, 468020]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 0, 0, 126335, 531077]</td>\n",
              "      <td>[0, 0, 0, 2478, 992]</td>\n",
              "      <td>[0, 0, 126335, 531077, 253667]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 0, 126335, 1338469, 2261828]</td>\n",
              "      <td>[0, 0, 2474, 1105, 1105]</td>\n",
              "      <td>[0, 126335, 1338469, 2261828, 1846462]</td>\n",
              "      <td>[0, 0, 4, 8, 10]</td>\n",
              "      <td>[0, 0, 8, 8, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 126335, 180014, 833666]</td>\n",
              "      <td>[0, 0, 2458, 1119, 735]</td>\n",
              "      <td>[0, 126335, 180014, 833666, 640839]</td>\n",
              "      <td>[0, 0, 10, 10, 10]</td>\n",
              "      <td>[0, 0, 7, 8, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>[0, 0, 0, 126335, 1367424]</td>\n",
              "      <td>[0, 0, 0, 2195, 1217]</td>\n",
              "      <td>[0, 0, 126335, 1367424, 423547]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>[0, 0, 0, 139086, 141300]</td>\n",
              "      <td>[0, 0, 0, 2195, 1497]</td>\n",
              "      <td>[0, 0, 139086, 141300, 921642]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>[146684, 2340200, 731134, 2411704, 2864831]</td>\n",
              "      <td>[2195, 678, 648, 615, 551]</td>\n",
              "      <td>[2340200, 731134, 2411704, 2864831, 349579]</td>\n",
              "      <td>[10, 8, 10, 10, 10]</td>\n",
              "      <td>[8, 0, 7, 8, 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>[124204, 330238, 1871026, 1056174, 2686655]</td>\n",
              "      <td>[2194, 999, 605, 571, 509]</td>\n",
              "      <td>[330238, 1871026, 1056174, 2686655, 1603811]</td>\n",
              "      <td>[10, 10, 10, 8, 10]</td>\n",
              "      <td>[8, 8, 8, 8, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>[330238, 1871026, 1056174, 2686655, 1603811]</td>\n",
              "      <td>[999, 605, 571, 509, 495]</td>\n",
              "      <td>[1871026, 1056174, 2686655, 1603811, 2488048]</td>\n",
              "      <td>[10, 10, 8, 10, 8]</td>\n",
              "      <td>[8, 8, 8, 8, 8]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>72 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         item_id  ...          reviews\n",
              "0                      [0, 0, 0, 126335, 132738]  ...  [0, 0, 0, 8, 8]\n",
              "1                      [0, 0, 0, 125564, 240137]  ...  [0, 0, 0, 8, 8]\n",
              "2                      [0, 0, 0, 126335, 531077]  ...  [0, 0, 0, 8, 8]\n",
              "3               [0, 0, 126335, 1338469, 2261828]  ...  [0, 0, 8, 8, 3]\n",
              "4                 [0, 0, 126335, 180014, 833666]  ...  [0, 0, 7, 8, 7]\n",
              "..                                           ...  ...              ...\n",
              "67                    [0, 0, 0, 126335, 1367424]  ...  [0, 0, 0, 8, 8]\n",
              "68                     [0, 0, 0, 139086, 141300]  ...  [0, 0, 0, 8, 8]\n",
              "69   [146684, 2340200, 731134, 2411704, 2864831]  ...  [8, 0, 7, 8, 6]\n",
              "70   [124204, 330238, 1871026, 1056174, 2686655]  ...  [8, 8, 8, 8, 8]\n",
              "71  [330238, 1871026, 1056174, 2686655, 1603811]  ...  [8, 8, 8, 8, 8]\n",
              "\n",
              "[72 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R490Q-rCgIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807f32ea-f798-4e33-9fd8-b3b0290bf4ef"
      },
      "source": [
        "#index to rating\n",
        "all_input_rat = np.array([i for i in training_set[\"rating\"].values]).flatten()\n",
        "all_targettt_items = np.array([i for i in training_set[\"target\"].values]).flatten()\n",
        "all_items_rat = np.concatenate((all_input_rat, all_targettt_items))\n",
        "unique_itemsr = np.unique(all_items_rat)\n",
        "item_to_rating = {rating:i for i, rating in enumerate(unique_itemsr)}\n",
        "index_to_rating = {v:k for k,v in item_to_rating.items()}  \n",
        "index_to_rating[70]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-4M-2FDB4U-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f00a88-1f3c-4016-b7ed-0acf34b951eb"
      },
      "source": [
        "#index to review\n",
        "all_input_rev = np.array([i for i in training_set[\"reviews\"].values]).flatten()\n",
        "all_targett_items = np.array([i for i in training_set[\"target\"].values]).flatten()\n",
        "all_items_rev = np.concatenate((all_input_rev, all_targett_items))\n",
        "unique_itemsrv = np.unique(all_items_rev)\n",
        "item_to_review = {review:i for i, review in enumerate(unique_itemsrv)}\n",
        "index_to_review = {v:k for k,v in item_to_review.items()}  \n",
        "index_to_review[70]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151551"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PceSQd7I7X8R"
      },
      "source": [
        "**The new features you have chosen and why**\n",
        "\n",
        " *The new chosen features are reviews and rating.\n",
        "Rating alone was not providing a complete understanding of users feedback, where I believe that a score of 1 to 10 is not fully reliable to understand consumer satisfaction, providing possibly misleading results.\n",
        "I obtained new insight on reviews, understanding if they were positive or negative. In this way, the model has a deeper understanding of users feedbacks.*\n",
        "\n",
        "**How you have integrated them into the model**\n",
        "\n",
        "*I used an RNN trained on movie reviews to understand if reviews were positive or not. Then I appended it as a new column. I bucketed reviews and rating and trained the model with these two new features.*\n",
        "\n",
        "**Have they improved performance?**\n",
        "\n",
        "*The model might be capable to provide quality to quantitative insights regarding users satisfaction.*"
      ]
    }
  ]
}