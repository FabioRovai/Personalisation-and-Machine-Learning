{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 7 - miao.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabioRovai/Personalisation-and-Machine-Learning/blob/main/Assesment3(week7).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqNzDzXfVn0z"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0VlgRzmKx8S",
        "outputId": "01e66dc4-1518-4fe8-c93f-efa945acf5b3"
      },
      "source": [
        "\n",
        "#I implemented an RNN trained on Movies classification tasks to evaluate if these reviews where good or not.\n",
        "#Also, because the module was working just fine, I stored it in a .py file, keeping this notebook as clean as possible.\n",
        "!wget 'https://victorzhou.com/movie-reviews-dataset.zip' -qq\n",
        "!unzip -qq '/content/movie-reviews-dataset.zip' \n",
        "!pip install deepctr -qq\n",
        "#move to main colab directory\n",
        "%cd '/content/'\n",
        "#I did a module conversion of this tutorial https://victorzhou.com/blog/keras-rnn-tutorial/ and stored it into a git.\n",
        "!wget 'https://raw.githubusercontent.com/FabioRovai/test7/main/untitled49.py'\n",
        "#change into correct directory, otherwse manually change train and test\n",
        "%cd '/content/movie-reviews-dataset'\n",
        "#run the module\n",
        "%run '/content/untitled49.py'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 122kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 18.2MB/s \n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h/content\n",
            "--2021-06-07 14:02:57--  https://raw.githubusercontent.com/FabioRovai/test7/main/untitled49.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2710 (2.6K) [text/plain]\n",
            "Saving to: ‘untitled49.py’\n",
            "\n",
            "untitled49.py       100%[===================>]   2.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-06-07 14:02:57 (35.8 MB/s) - ‘untitled49.py’ saved [2710/2710]\n",
            "\n",
            "/content/movie-reviews-dataset\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "b'I imagine that the young people involved in the making of \"Necromancy\" (aka \"The Witching\" plus a bunch of other titles) must have felt a little weird being on the set of a horror movie with the man who: participated with John Houseman in the production of a proletarian play (\"The Cradle Will Rock\"); scared people into thinking that aliens were invading (\"The War of the Worlds\"); and directed and starred in the greatest movie of all time (\"Citizen Kane\"). And now Orson Welles was starring in a third-rate flick about a satanic cult.  There\\'s basically nothing creative about this movie. Lots of nudity, but the background music always proves really distracting. Even if the movie wasn\\'t particularly predictable, it still wasn\\'t worth seeing. How low Welles had sunk. Fortunately, over the final thirteen years of his life, he narrated the documentary \"Bugs Bunny Superstar\" (about the Warner Bros. cartoons of the 1940s) and hosted the documentary \"The Man who Saw Tomorrow\" (about Nostradamus). I recommend those two, but not this one. Just avoid it.  Also starring Pamela Franklin and Michael Ontkean.'\n",
            "0\n",
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 33s 34ms/step - loss: 0.5462 - accuracy: 0.7162\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.4489 - accuracy: 0.7953\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.4125 - accuracy: 0.8118\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 0.3891 - accuracy: 0.8267\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 26s 34ms/step - loss: 0.3674 - accuracy: 0.8368\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.3510 - accuracy: 0.8478\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.3352 - accuracy: 0.8556\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.3181 - accuracy: 0.8629\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.3006 - accuracy: 0.8712\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.2957 - accuracy: 0.8741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNFgLUnGK7Ae",
        "outputId": "0914bfc9-81d3-4a04-fec5-e983cc7c38e2"
      },
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 14s 17ms/step - loss: 0.5381 - accuracy: 0.7735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5381135940551758, 0.7734799981117249]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTzjpDFjLBJm",
        "outputId": "e612e2e6-e4ad-454e-f822-919f71b6532d"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "##Load the data \n",
        "#If raw token expires upload manually upload the dataset, is a bug or a security protocol from git.arts\n",
        "df = pd.read_json(\"https://git.arts.ac.uk/raw/lmccallum/Personalisation/master/data/renttherunway_cleaned.json?token=AAAAAH7XGLI4G4S6DYUDL5DAYPTSO\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "#df[['review_text']]\n",
        "#Select random result \n",
        "foo=df[['review_text']].sample()\n",
        "print (foo)\n",
        "print(model.predict([foo]))\n",
        "foo=df[['review_text']]\n",
        "#creating a new section for the review scores\n",
        "df[\"review_score\"] = model.predict([foo]) \n",
        "#print(df[\"review_score\"].describe())\n",
        "#df[\"rating\"].describe()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                                                                                                                       review_text\n",
            "16281  This dress is great for us pear-shaped women with big hips, big butt, and heavy thighs.  I loved how this dress made me look smaller and I received a lot of compliments.  \n",
            "[[0.97447294]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "DGWUqGZNVn0z",
        "outputId": "462026e1-d3a1-4146-ab31-2bc2a2768c9d"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fit</th>\n",
              "      <th>user_id</th>\n",
              "      <th>bust size</th>\n",
              "      <th>item_id</th>\n",
              "      <th>weight</th>\n",
              "      <th>rating</th>\n",
              "      <th>rented for</th>\n",
              "      <th>review_text</th>\n",
              "      <th>body type</th>\n",
              "      <th>review_summary</th>\n",
              "      <th>category</th>\n",
              "      <th>height</th>\n",
              "      <th>size</th>\n",
              "      <th>age</th>\n",
              "      <th>review_date</th>\n",
              "      <th>review_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fit</td>\n",
              "      <td>420272</td>\n",
              "      <td>34d</td>\n",
              "      <td>2260466</td>\n",
              "      <td>137lbs</td>\n",
              "      <td>10.0</td>\n",
              "      <td>vacation</td>\n",
              "      <td>An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.</td>\n",
              "      <td>hourglass</td>\n",
              "      <td>So many compliments!</td>\n",
              "      <td>romper</td>\n",
              "      <td>5' 8\"</td>\n",
              "      <td>14</td>\n",
              "      <td>28.0</td>\n",
              "      <td>April 20, 2016</td>\n",
              "      <td>0.961109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fit</td>\n",
              "      <td>273551</td>\n",
              "      <td>34b</td>\n",
              "      <td>153475</td>\n",
              "      <td>132lbs</td>\n",
              "      <td>10.0</td>\n",
              "      <td>other</td>\n",
              "      <td>I rented this dress for a photo shoot. The theme was \"Hollywood Glam and Big Beautiful Hats\". The dress was very comfortable and easy to move around in. It is definitely on my list to rent again for another formal event.</td>\n",
              "      <td>straight &amp; narrow</td>\n",
              "      <td>I felt so glamourous!!!</td>\n",
              "      <td>gown</td>\n",
              "      <td>5' 6\"</td>\n",
              "      <td>12</td>\n",
              "      <td>36.0</td>\n",
              "      <td>June 18, 2013</td>\n",
              "      <td>0.972715</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fit  user_id bust size  item_id  ... size   age     review_date review_score\n",
              "0  fit   420272       34d  2260466  ...   14  28.0  April 20, 2016     0.961109\n",
              "1  fit   273551       34b   153475  ...   12  36.0   June 18, 2013     0.972715\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36ZrCy3XVn00",
        "outputId": "587879aa-1154-4f08-e15a-e3215d81b8fe"
      },
      "source": [
        "#How many interactions in total, how many unique users, how many unique items \n",
        "len(df), len(df[\"user_id\"].unique()), len(df[\"item_id\"].unique())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(192544, 105571, 5850)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-WIm_OjVn01"
      },
      "source": [
        "### Dates \n",
        "\n",
        "Since sequence is important to us, we're going to need to sort the data by review date. Currently, ``Pandas`` sees this column as an ``object``, so we'll use ``pd.to_datetime()`` to convert the string to a date. We can then sort it, and do maths with it! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Kz4TpTVn01"
      },
      "source": [
        "#Parse the dates\n",
        "df[\"review_date\"] = pd.to_datetime(df[\"review_date\"])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9o07scVn01",
        "outputId": "20cc8bb2-f05f-4606-d19d-1c1bc5d50988"
      },
      "source": [
        "#Confirm the data types are correct\n",
        "df.dtypes"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fit                       object\n",
              "user_id                    int64\n",
              "bust size                 object\n",
              "item_id                    int64\n",
              "weight                    object\n",
              "rating                   float64\n",
              "rented for                object\n",
              "review_text               object\n",
              "body type                 object\n",
              "review_summary            object\n",
              "category                  object\n",
              "height                    object\n",
              "size                       int64\n",
              "age                      float64\n",
              "review_date       datetime64[ns]\n",
              "review_score             float32\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly3iqigAVn02"
      },
      "source": [
        "#Sort\n",
        "df = df.sort_values(by=\"review_date\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt1YPTcCRVYh"
      },
      "source": [
        "df_copy=df.copy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYz3EXXiVn02"
      },
      "source": [
        "### Getting the User Sequence \n",
        "\n",
        "Currently, our dataset has each item on a separate row. In order to get the sequence of purchase for each user, we need to format the data. \n",
        "\n",
        "We also take the date in its absolute form and change it to ``days since end of dataset`` (the most recent rental being ``0 days``)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uANZkinzVn03"
      },
      "source": [
        "users = df[\"user_id\"].unique()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k78eEtCVn03"
      },
      "source": [
        "data = []\n",
        "last_date = df[\"review_date\"].max()\n",
        "\n",
        "for user in users:\n",
        "    ##Get all the items for that user\n",
        "    rows = df[df[\"user_id\"]==user]\n",
        "    \n",
        "    #Get all the item_ids\n",
        "    items = rows[\"item_id\"]\n",
        "    #get all the review score\n",
        "    review_score = rows[\"review_score\"]\n",
        "    #get all the rating\n",
        "    rating   = rows[\"rating\"]\n",
        " \n",
        "    \n",
        "    #Get all the dates\n",
        "    days_since_now = (last_date - rows[\"review_date\"])\n",
        "    days_since_now = np.array([i.days for i in days_since_now])\n",
        "    \n",
        "    #Collect into a dictionary for each user\n",
        "    data.append({\"item_id\":items.values,\"nb_days\":days_since_now, \"rating\":rating.values, \"reviews\":review_score.values })"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSWTK-4FVn03"
      },
      "source": [
        "#Convert to a dataframe and save\n",
        "data = pd.DataFrame(data)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-L8cZBTVMSb"
      },
      "source": [
        "magic=data.copy()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRRvy-6pVAa5"
      },
      "source": [
        "#adjusting notation\n",
        "magic[\"reviews\"] = 10 * magic[\"reviews\"]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxO4Fkg5Vn04"
      },
      "source": [
        "#data\n",
        "#magic\n",
        "data=magic.copy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "59spVTvrXrp7",
        "outputId": "cc439269-3cbf-4170-b6ae-819faa617cab"
      },
      "source": [
        "magic"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>nb_days</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[125564, 183200]</td>\n",
              "      <td>[2623, 1383]</td>\n",
              "      <td>[10.0, 10.0]</td>\n",
              "      <td>[9.750337, 0.28816873]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[126335, 132738, 130259]</td>\n",
              "      <td>[2520, 2096, 1745]</td>\n",
              "      <td>[8.0, 10.0, 10.0]</td>\n",
              "      <td>[9.907851, 8.0528345, 8.4637785]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[126335]</td>\n",
              "      <td>[2511]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[9.75249]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[125564, 240137, 468020]</td>\n",
              "      <td>[2510, 848, 848]</td>\n",
              "      <td>[10.0, 10.0, 10.0]</td>\n",
              "      <td>[9.751079, 7.4201574, 9.753986]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[190529]</td>\n",
              "      <td>[2500]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[6.6679835]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105566</th>\n",
              "      <td>[454564]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[8.0]</td>\n",
              "      <td>[7.35361]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105567</th>\n",
              "      <td>[1498329]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[9.747259]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105568</th>\n",
              "      <td>[2835159]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[9.699287]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105569</th>\n",
              "      <td>[1969604]</td>\n",
              "      <td>[3]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[6.2292433]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105570</th>\n",
              "      <td>[2726766]</td>\n",
              "      <td>[1]</td>\n",
              "      <td>[10.0]</td>\n",
              "      <td>[6.80462]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>105571 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                         item_id  ...                           reviews\n",
              "0               [125564, 183200]  ...            [9.750337, 0.28816873]\n",
              "1       [126335, 132738, 130259]  ...  [9.907851, 8.0528345, 8.4637785]\n",
              "2                       [126335]  ...                         [9.75249]\n",
              "3       [125564, 240137, 468020]  ...   [9.751079, 7.4201574, 9.753986]\n",
              "4                       [190529]  ...                       [6.6679835]\n",
              "...                          ...  ...                               ...\n",
              "105566                  [454564]  ...                         [7.35361]\n",
              "105567                 [1498329]  ...                        [9.747259]\n",
              "105568                 [2835159]  ...                        [9.699287]\n",
              "105569                 [1969604]  ...                       [6.2292433]\n",
              "105570                 [2726766]  ...                         [6.80462]\n",
              "\n",
              "[105571 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFn93EVHVn04"
      },
      "source": [
        "### Make the Dataset \n",
        "\n",
        "Now we need to make the actual **training set** that we will use for our model. \n",
        "\n",
        "Remember, the purpose of this model is to learn to predict the **next item rented in the sequence**. So for our input we will have \n",
        "\n",
        " * [item1, item2, item3, item4, ..., itemN]\n",
        " \n",
        " * [days1, days2, days3, days4, ..., daysN]\n",
        " \n",
        " \n",
        "And for our output we will have **the next item in the sequence** \n",
        "  \n",
        " \n",
        " * [itemN+1]\n",
        " \n",
        " \n",
        "We also need our sequences to all be **of equal length**, and because this dataset doesn't have loads of really long sequences, we don't want to throw away stuff below a threshold! So, instead we **zero padd** the end of each sequence if its not as long as the maximum length we have picked (in this case ``5``)\n",
        "\n",
        "* [0, 0, item1, item2, ..., itemN]\n",
        " \n",
        "* [0, 0, days1, days2, ..., daysN]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK2CLdiHVn05"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#!!!"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZs-gkMLVn05"
      },
      "source": [
        "#split into examples of 5 last things + 1, if less than 5, zero pad\n",
        "training_set = []\n",
        "max_len = 5\n",
        "for _, user in data.iterrows():\n",
        "    i = 0\n",
        "    #Get all items for each user \n",
        "    items = user[\"item_id\"]\n",
        "    num_items = len(items)\n",
        "    #If only one item purchased, there is no sequence! We need at least 2\n",
        "    #I've found better accuracy limiting to min 3\n",
        "    if num_items > 2:\n",
        "        nb_days = np.array(user[\"nb_days\"])\n",
        "        rating = np.array(user[\"rating\"])\n",
        "        review_score = np.array(user[\"reviews\"])\n",
        "        end = False\n",
        "        #Cycle over items, taking windows of 5 and moving forwards by 1 each time\n",
        "        while not end:\n",
        "            \n",
        "            #If we're off the end of the list, break out of the loop\n",
        "            target = i + max_len\n",
        "            if target >= num_items - 1:\n",
        "                end = True\n",
        "                target = num_items - 1\n",
        "            \n",
        "            #Get the input item and day features, zero padding\n",
        "            input_items = pad_sequences([items[i:target]], max_len)[0]\n",
        "            days = pad_sequences([nb_days[i:target]], max_len)[0]\n",
        "            input_rating = pad_sequences([rating[i:target]], max_len)[0]\n",
        "            input_rewiews = pad_sequences([review_score[i:target]], max_len)[0]\n",
        "            \n",
        "            #Get the adjusted seqeunce (shifted by one) for the target\n",
        "            target_items = np.concatenate((input_items[1:],[items[target]]))\n",
        "            #target_items = [items[target]]\n",
        "            \n",
        "            #Add to the dataset\n",
        "            row = {\n",
        "                \"item_id\":input_items,\n",
        "                \"nb_days\":days,\n",
        "                \"target\":target_items,\n",
        "                \"rating\":input_rating,\n",
        "                \"reviews\":input_rewiews,\n",
        "            }\n",
        "            training_set.append(row)\n",
        "            \n",
        "            #Increment pointer\n",
        "            i = i + 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yffqJ6T7Vn07"
      },
      "source": [
        "training_set = pd.DataFrame(training_set)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv7UH7IQVn07",
        "outputId": "f66158b5-eb3d-43d1-ef79-3012da8296b2"
      },
      "source": [
        "len(training_set)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34588"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "Fz2cSMLnVn08",
        "outputId": "715d08a2-e801-4a62-ed20-80c5b225edb5"
      },
      "source": [
        "training_set"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>nb_days</th>\n",
              "      <th>target</th>\n",
              "      <th>rating</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[0, 0, 0, 126335, 132738]</td>\n",
              "      <td>[0, 0, 0, 2520, 2096]</td>\n",
              "      <td>[0, 0, 126335, 132738, 130259]</td>\n",
              "      <td>[0, 0, 0, 8, 10]</td>\n",
              "      <td>[0, 0, 0, 9, 8]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[0, 0, 0, 125564, 240137]</td>\n",
              "      <td>[0, 0, 0, 2510, 848]</td>\n",
              "      <td>[0, 0, 125564, 240137, 468020]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 9, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[0, 0, 0, 126335, 531077]</td>\n",
              "      <td>[0, 0, 0, 2478, 992]</td>\n",
              "      <td>[0, 0, 126335, 531077, 253667]</td>\n",
              "      <td>[0, 0, 0, 10, 10]</td>\n",
              "      <td>[0, 0, 0, 9, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[0, 0, 126335, 1338469, 2261828]</td>\n",
              "      <td>[0, 0, 2474, 1105, 1105]</td>\n",
              "      <td>[0, 126335, 1338469, 2261828, 1846462]</td>\n",
              "      <td>[0, 0, 4, 8, 10]</td>\n",
              "      <td>[0, 0, 6, 7, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[0, 0, 126335, 180014, 833666]</td>\n",
              "      <td>[0, 0, 2458, 1119, 735]</td>\n",
              "      <td>[0, 126335, 180014, 833666, 640839]</td>\n",
              "      <td>[0, 0, 10, 10, 10]</td>\n",
              "      <td>[0, 0, 9, 9, 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34583</th>\n",
              "      <td>[0, 0, 0, 715164, 2720289]</td>\n",
              "      <td>[0, 0, 0, 4, 4]</td>\n",
              "      <td>[0, 0, 715164, 2720289, 2665815]</td>\n",
              "      <td>[0, 0, 0, 10, 8]</td>\n",
              "      <td>[0, 0, 0, 7, 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34584</th>\n",
              "      <td>[0, 0, 0, 872442, 322704]</td>\n",
              "      <td>[0, 0, 0, 4, 4]</td>\n",
              "      <td>[0, 0, 872442, 322704, 844580]</td>\n",
              "      <td>[0, 0, 0, 8, 8]</td>\n",
              "      <td>[0, 0, 0, 6, 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34585</th>\n",
              "      <td>[0, 0, 2893615, 2072280, 2675545]</td>\n",
              "      <td>[0, 0, 4, 4, 4]</td>\n",
              "      <td>[0, 2893615, 2072280, 2675545, 1434889]</td>\n",
              "      <td>[0, 0, 8, 8, 10]</td>\n",
              "      <td>[0, 0, 7, 7, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34586</th>\n",
              "      <td>[0, 0, 0, 1252971, 2945301]</td>\n",
              "      <td>[0, 0, 0, 4, 4]</td>\n",
              "      <td>[0, 0, 1252971, 2945301, 1793377]</td>\n",
              "      <td>[0, 0, 0, 10, 8]</td>\n",
              "      <td>[0, 0, 0, 7, 6]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34587</th>\n",
              "      <td>[0, 0, 0, 2237227, 2125981]</td>\n",
              "      <td>[0, 0, 0, 3, 3]</td>\n",
              "      <td>[0, 0, 2237227, 2125981, 2451150]</td>\n",
              "      <td>[0, 0, 0, 8, 10]</td>\n",
              "      <td>[0, 0, 0, 7, 7]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34588 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 item_id  ...          reviews\n",
              "0              [0, 0, 0, 126335, 132738]  ...  [0, 0, 0, 9, 8]\n",
              "1              [0, 0, 0, 125564, 240137]  ...  [0, 0, 0, 9, 7]\n",
              "2              [0, 0, 0, 126335, 531077]  ...  [0, 0, 0, 9, 7]\n",
              "3       [0, 0, 126335, 1338469, 2261828]  ...  [0, 0, 6, 7, 0]\n",
              "4         [0, 0, 126335, 180014, 833666]  ...  [0, 0, 9, 9, 6]\n",
              "...                                  ...  ...              ...\n",
              "34583         [0, 0, 0, 715164, 2720289]  ...  [0, 0, 0, 7, 6]\n",
              "34584          [0, 0, 0, 872442, 322704]  ...  [0, 0, 0, 6, 6]\n",
              "34585  [0, 0, 2893615, 2072280, 2675545]  ...  [0, 0, 7, 7, 7]\n",
              "34586        [0, 0, 0, 1252971, 2945301]  ...  [0, 0, 0, 7, 6]\n",
              "34587        [0, 0, 0, 2237227, 2125981]  ...  [0, 0, 0, 7, 7]\n",
              "\n",
              "[34588 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD363dqaVn08"
      },
      "source": [
        "## Feature Engineering \n",
        "\n",
        "### Item Ids\n",
        "\n",
        "Currently the ``item_ids`` are arbitrary and relate to the **Rent the Runway** catalogue. We are going to encode the in an **embedding** so need to make them indexes from ``0 -> num_items``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZzvB5eoVn09"
      },
      "source": [
        "#Get all the unique items from the data set\n",
        "#!!!\n",
        "all_input_items = np.array([i for i in training_set[\"item_id\"].values]).flatten()\n",
        "all_target_items = np.array([i for i in training_set[\"target\"].values]).flatten()\n",
        "all_items = np.concatenate((all_input_items, all_target_items))\n",
        "unique_items = np.unique(all_items)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snn-Hv7LVn09"
      },
      "source": [
        "#Make a look up dictionary from item_id to index\n",
        "item_to_index = {item_id:i for i, item_id in enumerate(unique_items)}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oJAPUNAVn09"
      },
      "source": [
        "#Swap out the new ids and convert to 2d arrays for use in training \n",
        "item_indexes = np.array([np.array([item_to_index[i] for i in ids], dtype=int) for ids in training_set[\"item_id\"]], dtype=int)\n",
        "target_indexes = np.array([np.array([item_to_index[i] for i in ids], dtype=int) for ids in training_set[\"target\"]], dtype=int)\n",
        "days = np.array([np.array([j for j in i],dtype=int) for i in training_set[\"nb_days\"]],dtype=int)\n",
        "rating = np.array([np.array([k for k in i],dtype=int) for i in training_set[\"rating\"]],dtype=int)\n",
        "reviews = np.array([np.array([l for l in i],dtype=int) for i in training_set[\"reviews\"]],dtype=int)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vhfFsz9Vn0-"
      },
      "source": [
        "### Bucketting Dates\n",
        "\n",
        "One interesting approach they have taken at Decathalon is to **bucket** days and then **learn an embedding**, almost treating it as a **categorical variable**. \n",
        "\n",
        "To do this, we use ``tf.keras.layers.experimental.preprocessing.Discretization()`` to separate the days into **100 equally spaced bins**, with one more for the **zero padding** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viEEgasSVn0-"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_PIPStPVn0-",
        "scrolled": true,
        "outputId": "a927e953-4732-464a-bc1c-da7f04350147"
      },
      "source": [
        "#Get min and max\n",
        "days_min = days.min()\n",
        "days_max = days.max()\n",
        "\n",
        "#Generate 100 equally spaced boundaries \n",
        "boundaries = list(np.linspace(days_min, days_max,100,dtype=int))\n",
        "boundaries.insert(0,0)\n",
        "boundaries[1] = 1\n",
        "\n",
        "#Bucket the day features\n",
        "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "    bins=boundaries)\n",
        "bucket_days = discretize_layer(days).numpy() #- 1\n",
        "bucket_days = bucket_days\n",
        "\n",
        "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "    bins=boundaries)\n",
        "bucket_rat = discretize_layer(rating).numpy() #- 1\n",
        "bucket_rat = bucket_rat\n",
        "\n",
        "\n",
        "discretize_layer = tf.keras.layers.experimental.preprocessing.Discretization(\n",
        "    bins=boundaries)\n",
        "bucket_rev = discretize_layer(reviews).numpy() #- 1\n",
        "bucket_rev = bucket_rev"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n",
            "WARNING:tensorflow:bins is deprecated, please use bin_boundaries or num_bins instead.\n",
            "WARNING:tensorflow:Using a while_loop for converting BoostedTreesBucketize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSqA57zMVn0_",
        "outputId": "3c38d163-0abc-4667-8808-adfc4c01c4f3"
      },
      "source": [
        "days"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, 2520, 2096],\n",
              "       [   0,    0,    0, 2510,  848],\n",
              "       [   0,    0,    0, 2478,  992],\n",
              "       ...,\n",
              "       [   0,    0,    4,    4,    4],\n",
              "       [   0,    0,    0,    4,    4],\n",
              "       [   0,    0,    0,    3,    3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "898dyXv2Vn0_",
        "outputId": "b11bfc12-4057-406e-ffef-6d07c51aa1c5"
      },
      "source": [
        "bucket_days"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, 100,  84],\n",
              "       [  0,   0,   0, 100,  35],\n",
              "       [  0,   0,   0,  99,  40],\n",
              "       ...,\n",
              "       [  0,   0,   2,   2,   2],\n",
              "       [  0,   0,   0,   2,   2],\n",
              "       [  0,   0,   0,   2,   2]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSMFQ185Vn1A"
      },
      "source": [
        "## Training and Validation Sets \n",
        "\n",
        "Next we split the data into training and validation sets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9TtuAOhVn1A"
      },
      "source": [
        "#Generate and shuffle the indexes\n",
        "total = len(training_set)\n",
        "all_indexes = np.arange(total)\n",
        "np.random.shuffle(all_indexes)\n",
        "\n",
        "#Split the indexes\n",
        "split = 0.9\n",
        "train_indexes = all_indexes[:int(total*split)]\n",
        "test_indexes = all_indexes[int(total*split):]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC15KNcbVn1B"
      },
      "source": [
        "#Make a dictionary for the inputs \n",
        "train_x = {'item_id': item_indexes[train_indexes],\n",
        "          'nb_days': bucket_days[train_indexes],\n",
        "           'rating': bucket_rat[train_indexes],\n",
        "           'reviews':bucket_rev[train_indexes]}\n",
        "\n",
        "test_x = {'item_id': item_indexes[test_indexes],\n",
        "          'nb_days': bucket_days[test_indexes],\n",
        "          'rating': bucket_rat[test_indexes],\n",
        "          'reviews':bucket_rev[test_indexes]}\n",
        "\n",
        "#Make an array for the outputs \n",
        "train_y = target_indexes[train_indexes]\n",
        "test_y = target_indexes[test_indexes]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-xAsUPwcWe0",
        "outputId": "74157261-b2f7-4612-862d-32cf738c30b7"
      },
      "source": [
        "test_y"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  63, 2811, 3126, 1680,   13],\n",
              "       [  34, 3537,  455,  791, 1609],\n",
              "       [1135, 3393,  614, 2861, 3080],\n",
              "       ...,\n",
              "       [2627, 3562, 2274, 2544,  546],\n",
              "       [   0,    0,  245,  122,  521],\n",
              "       [ 999,  536, 5052, 2333, 4150]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrcsLGBUVn1B",
        "outputId": "1ef1e2c9-3076-4a5b-a043-53cfca959bad"
      },
      "source": [
        "test_x"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'item_id': array([[ 125,   63, 2811, 3126, 1680],\n",
              "        [1634,   34, 3537,  455,  791],\n",
              "        [4502, 1135, 3393,  614, 2861],\n",
              "        ...,\n",
              "        [1128, 2627, 3562, 2274, 2544],\n",
              "        [   0,    0,    0,  245,  122],\n",
              "        [3183,  999,  536, 5052, 2333]]),\n",
              " 'nb_days': array([[55, 55, 46, 23, 23],\n",
              "        [29, 29,  8,  6,  2],\n",
              "        [ 8,  5,  5,  5,  4],\n",
              "        ...,\n",
              "        [11, 11,  9,  9,  9],\n",
              "        [ 0,  0,  0, 25, 25],\n",
              "        [16, 16,  5,  5,  5]], dtype=int32),\n",
              " 'rating': array([[2, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        ...,\n",
              "        [2, 2, 2, 2, 2],\n",
              "        [0, 0, 0, 2, 2],\n",
              "        [2, 2, 2, 2, 2]], dtype=int32),\n",
              " 'reviews': array([[2, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        [2, 2, 2, 2, 2],\n",
              "        ...,\n",
              "        [2, 2, 2, 2, 0],\n",
              "        [0, 0, 0, 2, 2],\n",
              "        [2, 2, 2, 2, 2]], dtype=int32)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXeOYNvVn1C"
      },
      "source": [
        "## Build and Train the Model \n",
        "\n",
        "Previously we have used ``Keras's Sequential API``, where we first make a model then **sequentially add layers to it** one by one. This works because each layer only has one input, and outputs directly into the next. \n",
        "\n",
        "However, whilst this is broadly true of our model, its not strictly true. Our **Embedding layers** both feed into the **Concatenate layer**. So instead, we will use the [Keras Functional API](https://keras.io/guides/functional_api/).\n",
        "\n",
        "As the documentation says \n",
        "\n",
        "```\n",
        "The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
        "```\n",
        "\n",
        "Essentially, when we build up the ``network graph``, instead of **adding things to the model**, we **specify input layer** we want this new layer to have. \n",
        "\n",
        "Here, we make a layer (``x``), and then a layer(``output``), specifying that ``x`` is in the input for ``output``\n",
        "\n",
        "```\n",
        "x = layers.Dense(64, activation=\"relu\")\n",
        "outputs = layers.Dense(10)(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoIw1OBBVn1C"
      },
      "source": [
        "#Model hyper parameters \n",
        "item_vocab_size = len(unique_items)\n",
        "hp = {\n",
        "    \"embedding_item\":100,\n",
        "    \"embedding_rating\":20,\n",
        "    \"embedding_reviews\":20,\n",
        "    \"embedding_nb_days\":20,\n",
        "    \"rnn_units_cat\":[1024,512],\n",
        "    \"learning_rate\":0.01\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMNgE5CfVn1C"
      },
      "source": [
        "### ``tf.keras.Input()``\n",
        "\n",
        "The [Input Layer](https://keras.io/api/layers/core_layers/input/) sits at the base of our ``Model`` and takes does what it says on the tin - Takes the input. \n",
        "\n",
        "We specfiy a dictionary of inputs to match our dictionaries we made in the **training set**. This means we have **two separate inputs**, that feed into **two separate embedding layers**.\n",
        "\n",
        "We give them a ``batch_input_shape`` of ``[None, max_len]``, so that it know each batch will be a sequence of ``max_len`` (in our ase 5) items, but the batch_size itself isn't decided until we ``compile()`` the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Epv29opVn1D"
      },
      "source": [
        "inputs = {}\n",
        "inputs['item_id'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                   name='item_id', dtype=tf.int32)\n",
        "\n",
        "# nb_days bucketized\n",
        "inputs['nb_days'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                   name='nb_days', dtype=tf.int32)\n",
        "#rating\n",
        "inputs['rating'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                name='rating', dtype=tf.int32)\n",
        "#reviews   \n",
        "inputs['reviews'] = tf.keras.Input(batch_input_shape=[None, max_len],\n",
        "                                   name='reviews', dtype=tf.int32)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL89VdErVn1D"
      },
      "source": [
        "### ``tf.keras.layers.Embedding()``\n",
        "\n",
        "Then we add the embedding layers, each time specifying which item from the **input dictionary** to take as input. \n",
        "\n",
        "The **item embedding** takes an input the size of the **number of unique items (vocab size)** and learns a mapping to a denser embedding of a given size. \n",
        "\n",
        "The **days embedding** takes an input the size of the **number of buckets + 1 (for zero padding)** and learns a mapping to a denser embedding of a given size. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOWrZiXwVn1E"
      },
      "source": [
        "embedding_item = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
        "                                           output_dim=hp.get('embedding_item'),\n",
        "                                           name='embedding_item'\n",
        "                                          )(inputs['item_id'])\n",
        "# nbins=100, +1 for zero padding\n",
        "embedding_nb_days = tf.keras.layers.Embedding(input_dim=100 + 1,\n",
        "                                              output_dim=hp.get('embedding_nb_days'),\n",
        "                                              name='embedding_nb_days'\n",
        "                                             )(inputs['nb_days'])\n",
        "\n",
        "\n",
        "embedding_rating = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
        "                                           output_dim=hp.get('embedding_rating'),\n",
        "                                           name='embedding_rating'\n",
        "                                          )(inputs['rating'])\n",
        "\n",
        "\n",
        "embedding_reviews = tf.keras.layers.Embedding(input_dim=item_vocab_size,\n",
        "                                           output_dim=hp.get('embedding_reviews'),\n",
        "                                           name='embedding_reviews'\n",
        "                                          )(inputs['reviews'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIwLV4SbsaSx",
        "outputId": "68e3d672-2eec-4f4b-e07f-e4c7511725bb"
      },
      "source": [
        "print(embedding_item,embedding_nb_days,embedding_rating,embedding_reviews)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 5, 100), dtype=tf.float32, name=None), name='embedding_item/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_item'\") KerasTensor(type_spec=TensorSpec(shape=(None, 5, 20), dtype=tf.float32, name=None), name='embedding_nb_days/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_nb_days'\") KerasTensor(type_spec=TensorSpec(shape=(None, 5, 20), dtype=tf.float32, name=None), name='embedding_rating/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_rating'\") KerasTensor(type_spec=TensorSpec(shape=(None, 5, 20), dtype=tf.float32, name=None), name='embedding_reviews/embedding_lookup/Identity_1:0', description=\"created by layer 'embedding_reviews'\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5h6cEYeVn1E"
      },
      "source": [
        "### `` tf.keras.layers.Concatenate()``\n",
        "\n",
        "We then concatentate embedding layers into one layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4RnK07sVn1F"
      },
      "source": [
        "# Concatenate embedding layers\n",
        "concat_embedding_input = tf.keras.layers.Concatenate(\n",
        " name='concat_embedding_input')([embedding_item, embedding_nb_days,embedding_rating, embedding_reviews  ]) # embedding_rating, embedding_reviews\n",
        "\n",
        "concat_embedding_input = tf.keras.layers.BatchNormalization(\n",
        " name='batchnorm_inputs')(concat_embedding_input)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v65054oVn1F"
      },
      "source": [
        "### LSTM Layers \n",
        "\n",
        "When then put in the ``tf.keras.layers.LSTM()`` layer, with a ``tf.keras.layers.BatchNormalization()`` either side. \n",
        "\n",
        "More on that in the lecture!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFRLrZQ2Vn1F"
      },
      "source": [
        "input_layer = concat_embedding_input\n",
        "\n",
        "for i, num_units in enumerate(hp.get('rnn_units_cat')):\n",
        "    \n",
        "    # LSTM layer\n",
        "    rnn = tf.keras.layers.LSTM(units=num_units,\n",
        "                                   return_sequences=True,\n",
        "                                   recurrent_initializer='glorot_normal',\n",
        "                                   name='LSTM_cat' + str(i)\n",
        "                                   )(input_layer)\n",
        "\n",
        "    rnn = tf.keras.layers.BatchNormalization(name='batchnorm_lstm' + str(i))(rnn)\n",
        "    \n",
        "    input_layer = rnn\n",
        "\n",
        "# create encoding padding mask\n",
        "encoding_padding_mask = tf.math.logical_not(tf.math.equal(inputs['item_id'], 0))\n",
        "\n",
        "# Self attention so key=value in inputs\n",
        "att = tf.keras.layers.Attention(use_scale=False, causal=True,\n",
        "                                name='attention')(inputs=[rnn, rnn],\n",
        "                                                  mask=[encoding_padding_mask,\n",
        "                                                        encoding_padding_mask])\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R0UKQi7Vn1G"
      },
      "source": [
        "### The Output\n",
        "\n",
        "Finally, we bring it all together in a ``tf.keras.layers.Dense()`` **softmax layer**. This means that the output of this layer will be \n",
        "\n",
        "``\n",
        "[batch_size x max_len x item_vocab_size]\n",
        "``\n",
        "\n",
        "Where each sequence in the batch is a ``[max_len x item_vocab_size]`` tensor telling us the probability of that item in the catalogue being next in the sequence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfBuqRjEVn1G"
      },
      "source": [
        "# Last layer is a fully connected one\n",
        "output = tf.keras.layers.Dense(item_vocab_size, activation = tf.nn.softmax, name='output')(att)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqvWaDxVVn1G"
      },
      "source": [
        "### Loss Function \n",
        "\n",
        "We write a custom loss function. This is necessary because we need to again mask out the 0s to stop the model optimising towards them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJBGTQC9Vn1G"
      },
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "def loss_function(real, pred):\n",
        "    loss = SparseCategoricalCrossentropy()(real, pred)\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXUIR2_cVn1H"
      },
      "source": [
        "### ``tf.keras.Model()``\n",
        "\n",
        "Finally, we're ready to join the ``Inputs`` and the ``Outputs`` into a ``Model()`` object and ``compile()``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF98oCMBVn1H"
      },
      "source": [
        "model = tf.keras.Model(inputs, output)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(hp.get('learning_rate')),\n",
        "    loss=loss_function,\n",
        "    metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU-xvJrXVn1H",
        "outputId": "a7d2b87b-e4d8-41c3-a2f4-715db59b2d4d"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "item_id (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "nb_days (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rating (InputLayer)             [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reviews (InputLayer)            [(None, 5)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_item (Embedding)      (None, 5, 100)       570100      item_id[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_nb_days (Embedding)   (None, 5, 20)        2020        nb_days[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_rating (Embedding)    (None, 5, 20)        114020      rating[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_reviews (Embedding)   (None, 5, 20)        114020      reviews[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concat_embedding_input (Concate (None, 5, 160)       0           embedding_item[0][0]             \n",
            "                                                                 embedding_nb_days[0][0]          \n",
            "                                                                 embedding_rating[0][0]           \n",
            "                                                                 embedding_reviews[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batchnorm_inputs (BatchNormaliz (None, 5, 160)       640         concat_embedding_input[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "LSTM_cat0 (LSTM)                (None, 5, 1024)      4853760     batchnorm_inputs[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batchnorm_lstm0 (BatchNormaliza (None, 5, 1024)      4096        LSTM_cat0[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "LSTM_cat1 (LSTM)                (None, 5, 512)       3147776     batchnorm_lstm0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.equal (TFOpLambda)      (None, 5)            0           item_id[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batchnorm_lstm1 (BatchNormaliza (None, 5, 512)       2048        LSTM_cat1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.logical_not (TFOpLambda (None, 5)            0           tf.math.equal[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention (Attention)           (None, 5, 512)       0           batchnorm_lstm1[0][0]            \n",
            "                                                                 batchnorm_lstm1[0][0]            \n",
            "                                                                 tf.math.logical_not[0][0]        \n",
            "                                                                 tf.math.logical_not[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "output (Dense)                  (None, 5, 5701)      2924613     attention[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,733,093\n",
            "Trainable params: 11,729,701\n",
            "Non-trainable params: 3,392\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua5xxDiIVn1I",
        "scrolled": true,
        "outputId": "5d768023-2130-49c9-cdfa-bd392d062085"
      },
      "source": [
        "history = model.fit(train_x,train_y,\n",
        "                    epochs=20, \n",
        "                    verbose=1,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(test_x, test_y))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "61/61 [==============================] - 14s 146ms/step - loss: 6.2207 - sparse_categorical_accuracy: 0.0022 - val_loss: 6.3555 - val_sparse_categorical_accuracy: 3.4691e-04\n",
            "Epoch 2/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 5.5485 - sparse_categorical_accuracy: 0.0111 - val_loss: 6.0281 - val_sparse_categorical_accuracy: 0.0019\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - 8s 128ms/step - loss: 4.8224 - sparse_categorical_accuracy: 0.0744 - val_loss: 5.5729 - val_sparse_categorical_accuracy: 0.0101\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 3.9716 - sparse_categorical_accuracy: 0.2208 - val_loss: 4.8806 - val_sparse_categorical_accuracy: 0.0683\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - 8s 126ms/step - loss: 3.2491 - sparse_categorical_accuracy: 0.3724 - val_loss: 4.1614 - val_sparse_categorical_accuracy: 0.2063\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 2.7342 - sparse_categorical_accuracy: 0.4819 - val_loss: 3.6258 - val_sparse_categorical_accuracy: 0.3435\n",
            "Epoch 7/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 2.3873 - sparse_categorical_accuracy: 0.5497 - val_loss: 3.4287 - val_sparse_categorical_accuracy: 0.4162\n",
            "Epoch 8/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 2.1214 - sparse_categorical_accuracy: 0.6051 - val_loss: 3.3239 - val_sparse_categorical_accuracy: 0.4505\n",
            "Epoch 9/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.9236 - sparse_categorical_accuracy: 0.6402 - val_loss: 3.3314 - val_sparse_categorical_accuracy: 0.4715\n",
            "Epoch 10/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.7646 - sparse_categorical_accuracy: 0.6680 - val_loss: 3.3296 - val_sparse_categorical_accuracy: 0.4888\n",
            "Epoch 11/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.6256 - sparse_categorical_accuracy: 0.6927 - val_loss: 3.3546 - val_sparse_categorical_accuracy: 0.5001\n",
            "Epoch 12/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.5182 - sparse_categorical_accuracy: 0.7110 - val_loss: 3.3995 - val_sparse_categorical_accuracy: 0.5058\n",
            "Epoch 13/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.4244 - sparse_categorical_accuracy: 0.7279 - val_loss: 3.4373 - val_sparse_categorical_accuracy: 0.5065\n",
            "Epoch 14/20\n",
            "61/61 [==============================] - 8s 128ms/step - loss: 1.3543 - sparse_categorical_accuracy: 0.7406 - val_loss: 3.5317 - val_sparse_categorical_accuracy: 0.5072\n",
            "Epoch 15/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.2934 - sparse_categorical_accuracy: 0.7511 - val_loss: 3.5817 - val_sparse_categorical_accuracy: 0.5089\n",
            "Epoch 16/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.2385 - sparse_categorical_accuracy: 0.7641 - val_loss: 3.6832 - val_sparse_categorical_accuracy: 0.5075\n",
            "Epoch 17/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.1875 - sparse_categorical_accuracy: 0.7743 - val_loss: 3.7543 - val_sparse_categorical_accuracy: 0.5105\n",
            "Epoch 18/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.1460 - sparse_categorical_accuracy: 0.7830 - val_loss: 3.7735 - val_sparse_categorical_accuracy: 0.5105\n",
            "Epoch 19/20\n",
            "61/61 [==============================] - 8s 127ms/step - loss: 1.1122 - sparse_categorical_accuracy: 0.7910 - val_loss: 3.8634 - val_sparse_categorical_accuracy: 0.5103\n",
            "Epoch 20/20\n",
            "61/61 [==============================] - 8s 126ms/step - loss: 1.0684 - sparse_categorical_accuracy: 0.8026 - val_loss: 3.9615 - val_sparse_categorical_accuracy: 0.5131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUOKqKRyVn1I"
      },
      "source": [
        "## Predictions \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58HPB7bjVn1J"
      },
      "source": [
        "#Predictions for whole test set\n",
        "results = model.predict(test_x)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxxYY1E6Vn1J",
        "outputId": "02e3ec1a-92e1-478d-9437-10cc8e8221e8"
      },
      "source": [
        "#exmples x max_len x vocab size\n",
        "results.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3459, 5, 5701)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o348KLAVVn1J",
        "outputId": "a5e746e8-a47e-4e56-b5d1-e654abefb5b0"
      },
      "source": [
        "#max_len x vocab size\n",
        "results[100].shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 5701)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM5h7viSVn1K",
        "outputId": "3cf25a4f-e284-406b-c455-d716db401da1"
      },
      "source": [
        "#Get index of highest prob in item vocab\n",
        "results[100][-1].sum()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzf6Q-SfVn1L",
        "outputId": "33d33311-a234-4731-c48a-84060d338ffc"
      },
      "source": [
        "results[-1].argmax()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDXrvl2IVn1L"
      },
      "source": [
        "index_to_item = {v:k for k,v in item_to_index.items()}    "
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBBClMgjVn1L",
        "outputId": "577d6ee8-0ed2-4727-922f-68777726ad6a"
      },
      "source": [
        "index_to_item[3174]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1698166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}